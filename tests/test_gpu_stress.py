#!/usr/bin/env python3
"""
Test GPU utilization with longer text generation to stress the GPU more
"""

import sys
import time
import subprocess
import threading
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

from src.cdr.ai.llm_client import LLMClient, LLMConfig, LLMProvider

def test_longer_generation():
    """Test with longer text generation to stress GPU"""
    print("üî• GPU Stress Test with Longer Generation")
    print("=" * 60)
    
    # Configure for longer generation
    config = LLMConfig(
        provider=LLMProvider.OLLAMA,
        model_name="llama3.1:8b",
        temperature=0.7,
        num_predict=512  # Much longer generation
    )
    
    client = LLMClient(config)
    
    # Monitor GPU during generation
    def monitor_gpu():
        max_util = 0
        for i in range(30):  # Monitor for 30 seconds
            try:
                result = subprocess.run([
                    'nvidia-smi', 
                    '--query-gpu=utilization.gpu,power.draw',
                    '--format=csv,noheader,nounits'
                ], capture_output=True, text=True, timeout=3)
                
                if result.returncode == 0:
                    parts = result.stdout.strip().split(', ')
                    if len(parts) >= 2:
                        gpu_util = int(parts[0])
                        power = float(parts[1])
                        max_util = max(max_util, gpu_util)
                        print(f"   GPU: {gpu_util:2d}% | Power: {power:4.1f}W | Max: {max_util:2d}%", end='\r')
                        
            except Exception:
                pass
            time.sleep(1)
        return max_util
    
    # Start monitoring in background
    monitoring = True
    max_utilization = [0]
    
    def gpu_monitor_thread():
        while monitoring:
            try:
                result = subprocess.run([
                    'nvidia-smi', 
                    '--query-gpu=utilization.gpu',
                    '--format=csv,noheader,nounits'
                ], capture_output=True, text=True, timeout=2)
                
                if result.returncode == 0:
                    gpu_util = int(result.stdout.strip())
                    max_utilization[0] = max(max_utilization[0], gpu_util)
                        
            except Exception:
                pass
            time.sleep(0.5)
    
    monitor_thread = threading.Thread(target=gpu_monitor_thread)
    monitor_thread.start()
    
    print("üß† Generating long response about air traffic control...")
    start_time = time.time()
    
    # Complex prompt for longer generation
    long_prompt = """
    Provide a comprehensive analysis of conflict detection and resolution in air traffic control systems. 
    Discuss the following aspects in detail:
    
    1. Aircraft separation standards and safety margins
    2. Conflict detection algorithms and prediction methods
    3. Resolution strategies including heading, altitude, and speed changes
    4. Wake turbulence considerations and aircraft classification
    5. Technology integration including radar, ADS-B, and automation systems
    6. Human factors in air traffic control decision making
    7. Future trends in automated conflict resolution
    
    Provide specific examples and technical details for each point.
    """
    
    try:
        response = client._call_llm(long_prompt.strip())
        end_time = time.time()
        
        print(f"\n‚úÖ Generation completed in {end_time - start_time:.1f} seconds")
        print(f"üìù Response length: {len(response)} characters")
        print(f"üî• Maximum GPU utilization: {max_utilization[0]}%")
        
        # Show sample of response
        if len(response) > 200:
            print(f"üìÑ Response preview: {response[:200]}...")
        else:
            print(f"üìÑ Full response: {response}")
            
    except Exception as e:
        print(f"‚ùå Generation failed: {e}")
    finally:
        monitoring = False
        monitor_thread.join(timeout=2)
    
    return max_utilization[0]


def check_ollama_gpu_layers():
    """Check if we can configure GPU layers for Ollama"""
    print("\nüîß Checking GPU Configuration Options")
    print("=" * 50)
    
    # Check current model info
    try:
        result = subprocess.run(['ollama', 'ps'], capture_output=True, text=True)
        if result.returncode == 0:
            print("üìä Current Ollama Status:")
            print(f"   {result.stdout.strip()}")
    except Exception as e:
        print(f"‚ùå Failed to get Ollama status: {e}")
    
    print("\nüí° GPU Optimization Tips:")
    print("   1. Model is using Q4_K_M quantization (4-bit)")
    print("   2. 53%/47% CPU/GPU split suggests partial GPU usage")
    print("   3. High memory usage (91.8%) but low compute utilization")
    print("   4. Consider using environment variables to force GPU layers")
    
    return True


def test_ollama_gpu_env_config():
    """Test with GPU environment configuration"""
    print("\nüß™ Testing GPU Environment Configuration")
    print("=" * 50)
    
    import os
    
    # Try to set GPU layers environment variable
    original_layers = os.environ.get('OLLAMA_GPU_LAYERS')
    
    print("üîß Setting OLLAMA_GPU_LAYERS=999 (force all layers to GPU)")
    os.environ['OLLAMA_GPU_LAYERS'] = '999'
    
    try:
        # Note: This won't affect the already loaded model
        # but shows how to configure for next model load
        print("‚ö†Ô∏è  Note: Changes require model reload to take effect")
        print("   To apply: ollama stop && ollama serve")
        
        # Check if environment variable is set
        layers = os.environ.get('OLLAMA_GPU_LAYERS')
        print(f"‚úÖ OLLAMA_GPU_LAYERS is now set to: {layers}")
        
    finally:
        # Restore original value
        if original_layers is not None:
            os.environ['OLLAMA_GPU_LAYERS'] = original_layers
        else:
            os.environ.pop('OLLAMA_GPU_LAYERS', None)


def main():
    """Run comprehensive GPU analysis"""
    print("üöÄ Comprehensive GPU Analysis for Ollama")
    print("=" * 70)
    
    # Test longer generation
    max_util = test_longer_generation()
    
    # Check configuration options
    check_ollama_gpu_layers()
    
    # Test environment configuration
    test_ollama_gpu_env_config()
    
    print("\nüèÅ GPU Analysis Summary")
    print("=" * 50)
    
    if max_util > 50:
        print("‚úÖ HIGH GPU utilization achieved during inference")
        print("   GPU is being effectively used for LLM inference")
    elif max_util > 20:
        print("‚ö†Ô∏è  MODERATE GPU utilization during inference")
        print("   GPU is used but may not be fully optimized")
    else:
        print("‚ùå LOW GPU utilization during inference")
        print("   GPU configuration may need optimization")
    
    print(f"\nüìä Results:")
    print(f"   Maximum GPU utilization: {max_util}%")
    print(f"   Model quantization: Q4_K_M (4-bit)")
    print(f"   GPU memory usage: ~92% (model loaded)")
    print(f"   CPU/GPU split: 53%/47%")
    
    print(f"\nüí° Recommendations:")
    if max_util < 30:
        print("   1. Consider setting OLLAMA_GPU_LAYERS=999")
        print("   2. Restart Ollama service after configuration")
        print("   3. Try with different model quantizations")
        print("   4. Monitor during longer, more complex generations")
    else:
        print("   ‚úÖ GPU configuration appears to be working well")
    
    return max_util > 10  # Success if we see some GPU usage


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
